{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este exemplo é baseado nos tutoriais disponíveis na página do TensorFlow (https://www.tensorflow.org/get_started/mnist/beginners e https://www.tensorflow.org/get_started/mnist/pros)\n",
    "e também no TensorBoard https://www.youtube.com/watch?v=eBbEDRsCmv4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#import TensorFlow and MNIST\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os\n",
    "\n",
    "# carrega os dados do MNIST com os labels no formato \"one-hot vector\"\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# definimos algumas constantes\n",
    "IMAGE_WIDTH = 28\n",
    "IMAGE_HEIGHT = 28\n",
    "IMAGE_CHANNELS = 1\n",
    "IMAGE_SIZE = IMAGE_WIDTH * IMAGE_HEIGHT * IMAGE_CHANNELS\n",
    "\n",
    "N_CLASSES = 10\n",
    "MAX_ITERS = 201\n",
    "BATCH_SIZE = 64\n",
    "LOGDIR='/home/vitor/mnist/'\n",
    "if not os.path.exists(LOGDIR):\n",
    "    os.makedirs(LOGDIR)\n",
    "n_samples = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 2.4436404705047607.\n",
      "Test Accuracy: 0.1083984375\n",
      "Iteration 10, loss = 2.040421962738037.\n",
      "Test Accuracy: 0.396484375\n",
      "Iteration 20, loss = 1.5820215940475464.\n",
      "Test Accuracy: 0.6484375\n",
      "Iteration 30, loss = 1.2205810546875.\n",
      "Test Accuracy: 0.7060546875\n",
      "Iteration 40, loss = 0.8770782947540283.\n",
      "Test Accuracy: 0.76171875\n",
      "Iteration 50, loss = 0.8602623343467712.\n",
      "Test Accuracy: 0.787109375\n",
      "Iteration 60, loss = 0.5099860429763794.\n",
      "Test Accuracy: 0.798828125\n",
      "Iteration 70, loss = 0.6948658227920532.\n",
      "Test Accuracy: 0.8193359375\n",
      "Iteration 80, loss = 0.4978371858596802.\n",
      "Test Accuracy: 0.8232421875\n",
      "Iteration 90, loss = 0.6322829723358154.\n",
      "Test Accuracy: 0.81640625\n",
      "Iteration 100, loss = 0.668944239616394.\n",
      "Test Accuracy: 0.82421875\n",
      "Iteration 110, loss = 0.6026045083999634.\n",
      "Test Accuracy: 0.8359375\n",
      "Iteration 120, loss = 0.6109832525253296.\n",
      "Test Accuracy: 0.8388671875\n",
      "Iteration 130, loss = 0.5514481067657471.\n",
      "Test Accuracy: 0.8291015625\n",
      "Iteration 140, loss = 0.4958510994911194.\n",
      "Test Accuracy: 0.8388671875\n",
      "Iteration 150, loss = 0.5281027555465698.\n",
      "Test Accuracy: 0.8447265625\n",
      "Iteration 160, loss = 0.43607640266418457.\n",
      "Test Accuracy: 0.8515625\n",
      "Iteration 170, loss = 0.4698629379272461.\n",
      "Test Accuracy: 0.8486328125\n",
      "Iteration 180, loss = 0.5039570331573486.\n",
      "Test Accuracy: 0.8583984375\n",
      "Iteration 190, loss = 0.4936078190803528.\n",
      "Test Accuracy: 0.85546875\n",
      "Iteration 200, loss = 0.3950018584728241.\n",
      "Test Accuracy: 0.86328125\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "with tf.name_scope('input'):\n",
    "    # um placeholder para entradas (imagens) shape=(BATCH_SIZE, n_features)\n",
    "    x = tf.placeholder(tf.float32, shape=[None, IMAGE_SIZE], name='images')\n",
    "    # um placeholder para os labels, shape = (BATCH_SIZE, n_classes)\n",
    "    y = tf.placeholder(tf.float32, shape=[None, N_CLASSES], name=\"labels\")\n",
    "\n",
    "    # pesos W\n",
    "    W = tf.Variable(\n",
    "        tf.truncated_normal([IMAGE_SIZE, N_CLASSES], stddev=0.05),\n",
    "        name='Weights')\n",
    "    # viéses b\n",
    "    b = tf.Variable(tf.zeros([N_CLASSES]), name='Biases')\n",
    "\n",
    "    tf.summary.histogram(\"weights\", W)\n",
    "    tf.summary.histogram(\"biases\", b)\n",
    "\n",
    "    # modelo linear\n",
    "    logits = tf.matmul(x, W) + b\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    loss = loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    # SGD + momentum para otimização\n",
    "    learning_rate = 0.01\n",
    "    momentum = 0.9\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    # minimiza o valor da loss\n",
    "    optimizer = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"test_accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    test_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar(\"test_accuracy\", test_accuracy)\n",
    "\n",
    "summ = tf.summary.merge_all()    \n",
    "    \n",
    "# inicializa as variáveis globais\n",
    "sess.run(tf.global_variables_initializer())\n",
    "writer = tf.summary.FileWriter(LOGDIR + \"linear_regression\")\n",
    "writer.add_graph(sess.graph)\n",
    "for iteration in range(MAX_ITERS):\n",
    "    # obtém o próximo batch de imagens e labels\n",
    "    (images, labels) = mnist.train.next_batch(BATCH_SIZE)\n",
    "    \n",
    "    # executa uma iteração da otimização\n",
    "    (_, iter_loss) = sess.run([optimizer, loss], \n",
    "                          feed_dict={x: images,\n",
    "                                     y: labels}) \n",
    "    \n",
    "    # avalia o modelo a cada 10 iterações\n",
    "    if iteration % 10 == 0:\n",
    "        [test_acc, s] = sess.run([test_accuracy, summ], \n",
    "                                 feed_dict={x: mnist.test.images[:n_samples], \n",
    "                                            y: mnist.test.labels[:n_samples]})\n",
    "        print('Iteration {}, loss = {}.'.format(iteration, iter_loss))\n",
    "        print('Test Accuracy: {0}'.format(test_acc))\n",
    "        writer.add_summary(s, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 2.317106008529663.\n",
      "Test Accuracy: 0.12890625\n",
      "Iteration 10, loss = 2.3016648292541504.\n",
      "Test Accuracy: 0.1748046875\n",
      "Iteration 20, loss = 2.2366647720336914.\n",
      "Test Accuracy: 0.2099609375\n",
      "Iteration 30, loss = 2.1758460998535156.\n",
      "Test Accuracy: 0.3154296875\n",
      "Iteration 40, loss = 1.996772289276123.\n",
      "Test Accuracy: 0.4169921875\n",
      "Iteration 50, loss = 1.568583607673645.\n",
      "Test Accuracy: 0.4658203125\n",
      "Iteration 60, loss = 1.5448983907699585.\n",
      "Test Accuracy: 0.4833984375\n",
      "Iteration 70, loss = 1.195321798324585.\n",
      "Test Accuracy: 0.4921875\n",
      "Iteration 80, loss = 1.36623215675354.\n",
      "Test Accuracy: 0.4912109375\n",
      "Iteration 90, loss = 1.4441382884979248.\n",
      "Test Accuracy: 0.5751953125\n",
      "Iteration 100, loss = 1.1648666858673096.\n",
      "Test Accuracy: 0.6337890625\n",
      "Iteration 110, loss = 0.7468717694282532.\n",
      "Test Accuracy: 0.80078125\n",
      "Iteration 120, loss = 0.6994645595550537.\n",
      "Test Accuracy: 0.82421875\n",
      "Iteration 130, loss = 0.823643147945404.\n",
      "Test Accuracy: 0.845703125\n",
      "Iteration 140, loss = 0.518156886100769.\n",
      "Test Accuracy: 0.853515625\n",
      "Iteration 150, loss = 0.5145350694656372.\n",
      "Test Accuracy: 0.8935546875\n",
      "Iteration 160, loss = 0.2772626280784607.\n",
      "Test Accuracy: 0.912109375\n",
      "Iteration 170, loss = 0.3669617772102356.\n",
      "Test Accuracy: 0.935546875\n",
      "Iteration 180, loss = 0.5507206916809082.\n",
      "Test Accuracy: 0.904296875\n",
      "Iteration 190, loss = 0.29555562138557434.\n",
      "Test Accuracy: 0.93359375\n",
      "Iteration 200, loss = 0.12745149433612823.\n",
      "Test Accuracy: 0.9453125\n"
     ]
    }
   ],
   "source": [
    "# Adiciona Convolução\n",
    "def conv_layer(input, size_in, size_out, name=\"conv\", name2=\"pool\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([5, 5, size_in, size_out], stddev=0.05), name=\"Weights\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"Biases\")\n",
    "        conv = tf.nn.conv2d(input, w, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        relu = tf.nn.relu(conv + b)\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", relu)\n",
    "\n",
    "        return relu\n",
    "\n",
    "# Adiciona Max Pooling\n",
    "def max_pool(input, name):\n",
    "    with tf.name_scope(name):\n",
    "        return tf.nn.max_pool(input, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "# Adiciona totalmente conectada\n",
    "def fc_layer(input, size_in, size_out, name=\"fc\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.05), name=\"Weights\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"Biases\")\n",
    "        relu = tf.nn.relu(tf.matmul(input, w) + b)\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", relu)\n",
    "\n",
    "        return relu\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "# Setup placeholders\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784], name=\"images\")\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 10], name=\"labels\")\n",
    "\n",
    "# Redimensiona a imagem\n",
    "with tf.name_scope('input_reshape'):\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    tf.summary.image('input', x_image, 10)\n",
    "\n",
    "# convoluções e max poolings\n",
    "conv1 = conv_layer(x_image, 1, 32, \"conv1\", \"pool1\")\n",
    "conv1 = max_pool(conv1, \"pool1\")\n",
    "conv2 = conv_layer(conv1, 32, 64, \"conv2\", \"pool2\")\n",
    "conv2 = max_pool(conv2, \"pool2\")\n",
    "\n",
    "# transforma a imagem em um array\n",
    "with tf.name_scope('flatten'):\n",
    "    flattened = tf.reshape(conv2, [-1, 7 * 7 * 64])\n",
    "\n",
    "# totalmente conectada 1\n",
    "fc1 = fc_layer(flattened, 7 * 7 * 64, 1024, \"fc1\")\n",
    "\n",
    "with tf.name_scope('dropout'):\n",
    "    # dropout: elimina algumas unidades; veremos no final se der tempo\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "\n",
    "# totalmente conectada 2\n",
    "fc2 = fc_layer(fc1, 1024, 10, \"fc2\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=fc2, labels=y), name=\"cross_entropy\")\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    # SGD + momentum para otimização\n",
    "    learning_rate = 0.01\n",
    "    momentum = 0.9\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    # minimiza o valor da loss\n",
    "    optimizer = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"test_accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(fc2, 1), tf.argmax(y, 1))\n",
    "    test_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar(\"test_accuracy\", test_accuracy)\n",
    "\n",
    "summ = tf.summary.merge_all()    \n",
    "    \n",
    "# inicializa as variáveis globais\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# iniciar logger\n",
    "writer = tf.summary.FileWriter(LOGDIR + 'cnn')\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "for iteration in range(MAX_ITERS):\n",
    "    # obtém o próximo batch de imagens e labels\n",
    "    (images, labels) = mnist.train.next_batch(BATCH_SIZE)\n",
    "    \n",
    "    # executa uma iteração da otimização\n",
    "    (_, iter_loss) = sess.run([optimizer, loss], \n",
    "                          feed_dict={x: images,\n",
    "                                     y: labels,\n",
    "                                     keep_prob: 0.5})  \n",
    "    \n",
    "    # avalia o modelo a cada 10 iterações\n",
    "    if iteration % 10 == 0:\n",
    "        [test_acc, s] = sess.run([test_accuracy, summ], \n",
    "                                 feed_dict={x: mnist.test.images[:n_samples], \n",
    "                                            y: mnist.test.labels[:n_samples],\n",
    "                                            keep_prob: 1.0})\n",
    "        print('Iteration {}, loss = {}.'.format(iteration, iter_loss))\n",
    "        print('Test Accuracy: {0}'.format(test_acc))\n",
    "        writer.add_summary(s, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
