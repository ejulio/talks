{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este exemplo é baseado nos tutoriais disponíveis na página do TensorFlow (https://www.tensorflow.org/get_started/mnist/beginners e https://www.tensorflow.org/get_started/mnist/pros)\n",
    "e também no TensorBoard https://www.youtube.com/watch?v=eBbEDRsCmv4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#import TensorFlow and MNIST\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os\n",
    "\n",
    "# carrega os dados do MNIST com os labels no formato \"one-hot vector\"\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# definimos algumas constantes\n",
    "IMAGE_WIDTH = 28\n",
    "IMAGE_HEIGHT = 28\n",
    "IMAGE_CHANNELS = 1\n",
    "IMAGE_SIZE = IMAGE_WIDTH * IMAGE_HEIGHT * IMAGE_CHANNELS\n",
    "\n",
    "N_CLASSES = 10\n",
    "MAX_ITERS = 201\n",
    "BATCH_SIZE = 64\n",
    "LOGDIR='/home/vitor/mnist/'\n",
    "if not os.path.exists(LOGDIR):\n",
    "    os.makedirs(LOGDIR)\n",
    "n_samples = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/vitor/mnist/sprite_1024.png',\n",
       " <http.client.HTTPMessage at 0x7fdb4e32deb8>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "GITHUB_URL ='https://raw.githubusercontent.com/ejulio/talks/master/desmistificando-o-deep-learning-com-tensorflow/'\n",
    "\n",
    "### baixa sprites e labels para o projetor de embeddings ###\n",
    "urlretrieve(GITHUB_URL + 'labels_1024.tsv', LOGDIR + 'labels_1024.tsv')\n",
    "urlretrieve(GITHUB_URL + 'sprite_1024.png', LOGDIR + 'sprite_1024.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 2.3927762508392334.\n",
      "Test Accuracy: 0.1220703125\n",
      "Iteration 10, loss = 1.9464082717895508.\n",
      "Test Accuracy: 0.4208984375\n",
      "Iteration 20, loss = 1.4002333879470825.\n",
      "Test Accuracy: 0.70703125\n",
      "Iteration 30, loss = 1.144566297531128.\n",
      "Test Accuracy: 0.7333984375\n",
      "Iteration 40, loss = 0.8493523597717285.\n",
      "Test Accuracy: 0.771484375\n",
      "Iteration 50, loss = 0.8042867183685303.\n",
      "Test Accuracy: 0.80078125\n",
      "Iteration 60, loss = 0.7084593176841736.\n",
      "Test Accuracy: 0.8076171875\n",
      "Iteration 70, loss = 0.5597255229949951.\n",
      "Test Accuracy: 0.8193359375\n",
      "Iteration 80, loss = 0.5109997987747192.\n",
      "Test Accuracy: 0.8291015625\n",
      "Iteration 90, loss = 0.7448969483375549.\n",
      "Test Accuracy: 0.8408203125\n",
      "Iteration 100, loss = 0.4932386577129364.\n",
      "Test Accuracy: 0.845703125\n",
      "Iteration 110, loss = 0.5250451564788818.\n",
      "Test Accuracy: 0.853515625\n",
      "Iteration 120, loss = 0.6671329736709595.\n",
      "Test Accuracy: 0.8486328125\n",
      "Iteration 130, loss = 0.784690260887146.\n",
      "Test Accuracy: 0.8505859375\n",
      "Iteration 140, loss = 0.7149437665939331.\n",
      "Test Accuracy: 0.8427734375\n",
      "Iteration 150, loss = 0.5175398588180542.\n",
      "Test Accuracy: 0.8515625\n",
      "Iteration 160, loss = 0.39848941564559937.\n",
      "Test Accuracy: 0.8583984375\n",
      "Iteration 170, loss = 0.5146880149841309.\n",
      "Test Accuracy: 0.8564453125\n",
      "Iteration 180, loss = 0.5157380700111389.\n",
      "Test Accuracy: 0.845703125\n",
      "Iteration 190, loss = 0.41037505865097046.\n",
      "Test Accuracy: 0.8603515625\n",
      "Iteration 200, loss = 0.4173847436904907.\n",
      "Test Accuracy: 0.861328125\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "with tf.name_scope('input'):\n",
    "    # um placeholder para entradas (imagens) shape=(BATCH_SIZE, n_features)\n",
    "    x = tf.placeholder(tf.float32, shape=[None, IMAGE_SIZE], name='images')\n",
    "    # um placeholder para os labels, shape = (BATCH_SIZE, n_classes)\n",
    "    y = tf.placeholder(tf.float32, shape=[None, N_CLASSES], name=\"labels\")\n",
    "\n",
    "    # pesos W\n",
    "    W = tf.Variable(\n",
    "        tf.truncated_normal([IMAGE_SIZE, N_CLASSES], stddev=0.05),\n",
    "        name='Weights')\n",
    "    # viéses b\n",
    "    b = tf.Variable(tf.zeros([N_CLASSES]), name='Biases')\n",
    "\n",
    "    tf.summary.histogram(\"weights\", W)\n",
    "    tf.summary.histogram(\"biases\", b)\n",
    "\n",
    "    # modelo linear\n",
    "    logits = tf.matmul(x, W) + b\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    loss = loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    # SGD + momentum para otimização\n",
    "    learning_rate = 0.01\n",
    "    momentum = 0.9\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    # minimiza o valor da loss\n",
    "    optimizer = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"test_accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    test_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar(\"test_accuracy\", test_accuracy)\n",
    "\n",
    "summ = tf.summary.merge_all()    \n",
    "\n",
    "with tf.name_scope(\"embeddings\"):\n",
    "    embedding_input = x\n",
    "    embedding_size = IMAGE_SIZE\n",
    "    embedding = tf.Variable(tf.zeros([1024, embedding_size]), name=\"test_embedding\")\n",
    "    assignment = embedding.assign(embedding_input)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(LOGDIR + 'linear_regression_emb')\n",
    "    writer.add_graph(sess.graph)\n",
    "\n",
    "    config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n",
    "    embedding_config = config.embeddings.add()\n",
    "    embedding_config.tensor_name = embedding.name\n",
    "    embedding_config.sprite.image_path = LOGDIR + 'sprite_1024.png'\n",
    "    embedding_config.metadata_path = LOGDIR + 'labels_1024.tsv'\n",
    "    embedding_config.sprite.single_image_dim.extend([28, 28])\n",
    "    tf.contrib.tensorboard.plugins.projector.visualize_embeddings(writer, config)\n",
    "    \n",
    "for iteration in range(MAX_ITERS):\n",
    "    # obtém o próximo batch de imagens e labels\n",
    "    (images, labels) = mnist.train.next_batch(BATCH_SIZE)\n",
    "    \n",
    "    # executa uma iteração da otimização\n",
    "    (_, iter_loss) = sess.run([optimizer, loss], \n",
    "                          feed_dict={x: images,\n",
    "                                     y: labels}) \n",
    "    \n",
    "    # avalia o modelo a cada 10 iterações\n",
    "    if iteration % 10 == 0:\n",
    "        [test_acc, s] = sess.run([test_accuracy, summ], \n",
    "                                 feed_dict={x: mnist.test.images[:n_samples], \n",
    "                                            y: mnist.test.labels[:n_samples]})\n",
    "        print('Iteration {}, loss = {}.'.format(iteration, iter_loss))\n",
    "        print('Test Accuracy: {0}'.format(test_acc))\n",
    "        writer.add_summary(s, iteration)\n",
    "                                     \n",
    "    if iteration % 100 == 0:\n",
    "        sess.run(assignment, feed_dict={x: mnist.test.images[:n_samples], \n",
    "                                        y: mnist.test.labels[:n_samples]})\n",
    "        saver.save(sess, os.path.join(LOGDIR, \"linear_regression.ckpt\"), iteration)                                        \n",
    "                                     \n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 2.388707399368286.\n",
      "Test Accuracy: 0.0693359375\n",
      "Iteration 10, loss = 2.28298282623291.\n",
      "Test Accuracy: 0.099609375\n",
      "Iteration 20, loss = 2.23799467086792.\n",
      "Test Accuracy: 0.2060546875\n",
      "Iteration 30, loss = 2.201395034790039.\n",
      "Test Accuracy: 0.2705078125\n",
      "Iteration 40, loss = 2.1243033409118652.\n",
      "Test Accuracy: 0.3828125\n",
      "Iteration 50, loss = 1.5882071256637573.\n",
      "Test Accuracy: 0.453125\n",
      "Iteration 60, loss = 1.8074626922607422.\n",
      "Test Accuracy: 0.494140625\n",
      "Iteration 70, loss = 1.508946418762207.\n",
      "Test Accuracy: 0.525390625\n",
      "Iteration 80, loss = 1.5316241979599.\n",
      "Test Accuracy: 0.62890625\n",
      "Iteration 90, loss = 0.9368271827697754.\n",
      "Test Accuracy: 0.7412109375\n",
      "Iteration 100, loss = 0.6351594924926758.\n",
      "Test Accuracy: 0.748046875\n",
      "Iteration 110, loss = 0.8418694734573364.\n",
      "Test Accuracy: 0.7470703125\n",
      "Iteration 120, loss = 0.8123348355293274.\n",
      "Test Accuracy: 0.779296875\n",
      "Iteration 130, loss = 0.7865946292877197.\n",
      "Test Accuracy: 0.8310546875\n",
      "Iteration 140, loss = 0.6085005402565002.\n",
      "Test Accuracy: 0.845703125\n",
      "Iteration 150, loss = 0.48648107051849365.\n",
      "Test Accuracy: 0.8544921875\n",
      "Iteration 160, loss = 0.4772748351097107.\n",
      "Test Accuracy: 0.8466796875\n",
      "Iteration 170, loss = 0.32838115096092224.\n",
      "Test Accuracy: 0.85546875\n",
      "Iteration 180, loss = 0.5581278800964355.\n",
      "Test Accuracy: 0.8603515625\n",
      "Iteration 190, loss = 0.5342806577682495.\n",
      "Test Accuracy: 0.84765625\n",
      "Iteration 200, loss = 0.20638729631900787.\n",
      "Test Accuracy: 0.923828125\n"
     ]
    }
   ],
   "source": [
    "# Adiciona Convolução\n",
    "def conv_layer(input, size_in, size_out, name=\"conv\", name2=\"pool\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([5, 5, size_in, size_out], stddev=0.05), name=\"Weights\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"Biases\")\n",
    "        conv = tf.nn.conv2d(input, w, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        relu = tf.nn.relu(conv + b)\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", relu)\n",
    "\n",
    "        return relu\n",
    "\n",
    "# Adiciona Max Pooling\n",
    "def max_pool(input, name):\n",
    "    with tf.name_scope(name):\n",
    "        return tf.nn.max_pool(input, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "# Adiciona totalmente conectada\n",
    "def fc_layer(input, size_in, size_out, name=\"fc\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.05), name=\"Weights\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"Biases\")\n",
    "        relu = tf.nn.relu(tf.matmul(input, w) + b)\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", relu)\n",
    "\n",
    "        return relu\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "# Setup placeholders\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784], name=\"images\")\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 10], name=\"labels\")\n",
    "\n",
    "# Redimensiona a imagem\n",
    "with tf.name_scope('input_reshape'):\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    tf.summary.image('input', x_image, 10)\n",
    "\n",
    "# convoluções e max poolings\n",
    "conv1 = conv_layer(x_image, 1, 32, \"conv1\", \"pool1\")\n",
    "conv1 = max_pool(conv1, \"pool1\")\n",
    "conv2 = conv_layer(conv1, 32, 64, \"conv2\", \"pool2\")\n",
    "conv2 = max_pool(conv2, \"pool2\")\n",
    "\n",
    "# transforma a imagem em um array\n",
    "with tf.name_scope('flatten'):\n",
    "    flattened = tf.reshape(conv2, [-1, 7 * 7 * 64])\n",
    "\n",
    "# totalmente conectada 1\n",
    "fc1 = fc_layer(flattened, 7 * 7 * 64, 1024, \"fc1\")\n",
    "\n",
    "with tf.name_scope('dropout'):\n",
    "    # dropout: elimina algumas unidades; veremos no final se der tempo\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "\n",
    "# totalmente conectada 2\n",
    "fc2 = fc_layer(fc1, 1024, 10, \"fc2\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=fc2, labels=y), name=\"cross_entropy\")\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    # SGD + momentum para otimização\n",
    "    learning_rate = 0.01\n",
    "    momentum = 0.9\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    # minimiza o valor da loss\n",
    "    optimizer = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"test_accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(fc2, 1), tf.argmax(y, 1))\n",
    "    test_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar(\"test_accuracy\", test_accuracy)\n",
    "\n",
    "summ = tf.summary.merge_all()    \n",
    "    \n",
    "with tf.name_scope(\"embeddings\"):\n",
    "    embedding_input = fc1\n",
    "    embedding_size = 1024\n",
    "    embedding = tf.Variable(tf.zeros([1024, embedding_size]), name=\"test_embedding\")\n",
    "    assignment = embedding.assign(embedding_input)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(LOGDIR + 'cnn_emb')\n",
    "    writer.add_graph(sess.graph)\n",
    "\n",
    "    config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n",
    "    embedding_config = config.embeddings.add()\n",
    "    embedding_config.tensor_name = embedding.name\n",
    "    embedding_config.sprite.image_path = LOGDIR + 'sprite_1024.png'\n",
    "    embedding_config.metadata_path = LOGDIR + 'labels_1024.tsv'\n",
    "    embedding_config.sprite.single_image_dim.extend([28, 28])\n",
    "    tf.contrib.tensorboard.plugins.projector.visualize_embeddings(writer, config)\n",
    "\n",
    "for iteration in range(MAX_ITERS):\n",
    "    # obtém o próximo batch de imagens e labels\n",
    "    (images, labels) = mnist.train.next_batch(BATCH_SIZE)\n",
    "    \n",
    "    # executa uma iteração da otimização\n",
    "    (_, iter_loss) = sess.run([optimizer, loss], \n",
    "                          feed_dict={x: images,\n",
    "                                     y: labels,\n",
    "                                     keep_prob: 0.5})  \n",
    "    \n",
    "    # avalia o modelo a cada 10 iterações\n",
    "    if iteration % 10 == 0:\n",
    "        [test_acc, s] = sess.run([test_accuracy, summ], \n",
    "                                 feed_dict={x: mnist.test.images[:n_samples], \n",
    "                                            y: mnist.test.labels[:n_samples],\n",
    "                                            keep_prob: 1.0})\n",
    "        print('Iteration {}, loss = {}.'.format(iteration, iter_loss))\n",
    "        print('Test Accuracy: {0}'.format(test_acc))\n",
    "        writer.add_summary(s, iteration)\n",
    "        \n",
    "    if iteration % 100 == 0:\n",
    "        sess.run(assignment, feed_dict={x: mnist.test.images[:n_samples], \n",
    "                                        y: mnist.test.labels[:n_samples],\n",
    "                                        keep_prob: 1.0})\n",
    "        saver.save(sess, os.path.join(LOGDIR, \"cnn.ckpt\"), iteration)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
