{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este exemplo é baseado nos tutoriais disponíveis na página do TensorFlow (https://www.tensorflow.org/get_started/mnist/beginners e https://www.tensorflow.org/get_started/mnist/pros)\n",
    "e também no TensorBoard https://www.youtube.com/watch?v=eBbEDRsCmv4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#import TensorFlow and MNIST\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os\n",
    "\n",
    "# carrega os dados do MNIST com os labels no formato \"one-hot vector\"\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos algumas constantes\n",
    "IMAGE_WIDTH = 28\n",
    "IMAGE_HEIGHT = 28\n",
    "IMAGE_CHANNELS = 1\n",
    "IMAGE_SIZE = IMAGE_WIDTH * IMAGE_HEIGHT * IMAGE_CHANNELS\n",
    "\n",
    "N_CLASSES = 10\n",
    "MAX_ITERS = 201\n",
    "BATCH_SIZE = 64\n",
    "LOGDIR='/home/vitor/mnist/'\n",
    "if not os.path.exists(LOGDIR):\n",
    "    os.makedirs(LOGDIR)\n",
    "n_samples = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/vitor/mnist/sprite_1024.png',\n",
       " <http.client.HTTPMessage at 0x7fdb4e2e49e8>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "GITHUB_URL ='https://raw.githubusercontent.com/ejulio/talks/master/desmistificando-o-deep-learning-com-tensorflow/'\n",
    "\n",
    "### baixa sprites e labels para o projetor de embeddings ###\n",
    "urlretrieve(GITHUB_URL + 'labels_1024.tsv', LOGDIR + 'labels_1024.tsv')\n",
    "urlretrieve(GITHUB_URL + 'sprite_1024.png', LOGDIR + 'sprite_1024.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 2.3173325061798096.\n",
      "Test Accuracy: 0.1142578125\n",
      "Iteration 10, loss = 1.9900903701782227.\n",
      "Test Accuracy: 0.4462890625\n",
      "Iteration 20, loss = 1.3268506526947021.\n",
      "Test Accuracy: 0.689453125\n",
      "Iteration 30, loss = 1.0565403699874878.\n",
      "Test Accuracy: 0.7783203125\n",
      "Iteration 40, loss = 0.9338254332542419.\n",
      "Test Accuracy: 0.7744140625\n",
      "Iteration 50, loss = 0.8304336071014404.\n",
      "Test Accuracy: 0.79296875\n",
      "Iteration 60, loss = 0.8619265556335449.\n",
      "Test Accuracy: 0.814453125\n",
      "Iteration 70, loss = 0.615776777267456.\n",
      "Test Accuracy: 0.8330078125\n",
      "Iteration 80, loss = 0.7387078404426575.\n",
      "Test Accuracy: 0.830078125\n",
      "Iteration 90, loss = 0.4721797704696655.\n",
      "Test Accuracy: 0.8369140625\n",
      "Iteration 100, loss = 0.5873995423316956.\n",
      "Test Accuracy: 0.8515625\n",
      "Iteration 110, loss = 0.4997398555278778.\n",
      "Test Accuracy: 0.84765625\n",
      "Iteration 120, loss = 0.3879268765449524.\n",
      "Test Accuracy: 0.84375\n",
      "Iteration 130, loss = 0.4842590093612671.\n",
      "Test Accuracy: 0.8544921875\n",
      "Iteration 140, loss = 0.551626443862915.\n",
      "Test Accuracy: 0.8525390625\n",
      "Iteration 150, loss = 0.45176729559898376.\n",
      "Test Accuracy: 0.8564453125\n",
      "Iteration 160, loss = 0.4654099941253662.\n",
      "Test Accuracy: 0.8603515625\n",
      "Iteration 170, loss = 0.5229558944702148.\n",
      "Test Accuracy: 0.861328125\n",
      "Iteration 180, loss = 0.5780580043792725.\n",
      "Test Accuracy: 0.86328125\n",
      "Iteration 190, loss = 0.4306889474391937.\n",
      "Test Accuracy: 0.8642578125\n",
      "Iteration 200, loss = 0.6295022964477539.\n",
      "Test Accuracy: 0.861328125\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "with tf.name_scope('input'):\n",
    "    # um placeholder para entradas (imagens) shape=(BATCH_SIZE, n_features)\n",
    "    x = tf.placeholder(tf.float32, shape=[None, IMAGE_SIZE])\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    tf.summary.image('input', x_image, 10)\n",
    "    # um placeholder para os labels, shape = (BATCH_SIZE, n_classes)\n",
    "    y = tf.placeholder(tf.float32, shape=[None, N_CLASSES])    \n",
    "\n",
    "    # pesos W\n",
    "    W = tf.Variable(\n",
    "        tf.truncated_normal([IMAGE_SIZE, N_CLASSES], stddev = 0.05))\n",
    "    # viéses b\n",
    "    b = tf.Variable(tf.zeros([N_CLASSES]))\n",
    "\n",
    "    # modelo linear\n",
    "    logits = tf.matmul(x, W) + b\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    loss = loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    # SGD + momentum para otimização\n",
    "    learning_rate = 0.01\n",
    "    momentum = 0.9\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    # minimiza o valor da loss\n",
    "    optimizer = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"test_accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    test_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar(\"test_accuracy\", test_accuracy)\n",
    "\n",
    "summ = tf.summary.merge_all()    \n",
    "\n",
    "with tf.name_scope(\"embeddings\"):\n",
    "    embedding_input = x\n",
    "    embedding_size = IMAGE_SIZE\n",
    "    embedding = tf.Variable(tf.zeros([1024, embedding_size]), name=\"test_embedding\")\n",
    "    assignment = embedding.assign(embedding_input)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(LOGDIR + 'linear_regression_emb')\n",
    "    writer.add_graph(sess.graph)\n",
    "\n",
    "    config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n",
    "    embedding_config = config.embeddings.add()\n",
    "    embedding_config.tensor_name = embedding.name\n",
    "    embedding_config.sprite.image_path = LOGDIR + 'sprite_1024.png'\n",
    "    embedding_config.metadata_path = LOGDIR + 'labels_1024.tsv'\n",
    "    embedding_config.sprite.single_image_dim.extend([28, 28])\n",
    "    tf.contrib.tensorboard.plugins.projector.visualize_embeddings(writer, config)\n",
    "    \n",
    "for iteration in range(MAX_ITERS):\n",
    "    # obtém o próximo batch de imagens e labels\n",
    "    (images, labels) = mnist.train.next_batch(BATCH_SIZE)\n",
    "    \n",
    "    # executa uma iteração da otimização\n",
    "    (_, iter_loss) = sess.run([optimizer, loss], \n",
    "                          feed_dict={x: images,\n",
    "                                     y: labels}) \n",
    "    \n",
    "    # avalia o modelo a cada 10 iterações\n",
    "    if iteration % 10 == 0:\n",
    "        [test_acc, s] = sess.run([test_accuracy, summ], \n",
    "                                 feed_dict={x: mnist.test.images[:n_samples], \n",
    "                                            y: mnist.test.labels[:n_samples]})\n",
    "        print('Iteration {}, loss = {}.'.format(iteration, iter_loss))\n",
    "        print('Test Accuracy: {0}'.format(test_acc))\n",
    "        writer.add_summary(s, iteration)\n",
    "                                     \n",
    "    if iteration % 100 == 0:\n",
    "        sess.run(assignment, feed_dict={x: mnist.test.images[:n_samples], \n",
    "                                        y: mnist.test.labels[:n_samples]})\n",
    "        saver.save(sess, os.path.join(LOGDIR, \"linear_regression.ckpt\"), iteration)                                        \n",
    "                                     \n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 2.3959710597991943.\n",
      "Test Accuracy: 0.0703125\n",
      "Iteration 10, loss = 2.28951358795166.\n",
      "Test Accuracy: 0.1279296875\n",
      "Iteration 20, loss = 2.2511489391326904.\n",
      "Test Accuracy: 0.2021484375\n",
      "Iteration 30, loss = 2.076698064804077.\n",
      "Test Accuracy: 0.26171875\n",
      "Iteration 40, loss = 1.8775060176849365.\n",
      "Test Accuracy: 0.4697265625\n",
      "Iteration 50, loss = 1.5000818967819214.\n",
      "Test Accuracy: 0.6337890625\n",
      "Iteration 60, loss = 0.9770127534866333.\n",
      "Test Accuracy: 0.6787109375\n",
      "Iteration 70, loss = 0.9036784768104553.\n",
      "Test Accuracy: 0.76953125\n",
      "Iteration 80, loss = 0.6475479602813721.\n",
      "Test Accuracy: 0.8408203125\n",
      "Iteration 90, loss = 0.3419610261917114.\n",
      "Test Accuracy: 0.8828125\n",
      "Iteration 100, loss = 0.5540417432785034.\n",
      "Test Accuracy: 0.869140625\n",
      "Iteration 110, loss = 0.4219307601451874.\n",
      "Test Accuracy: 0.8994140625\n",
      "Iteration 120, loss = 0.31011688709259033.\n",
      "Test Accuracy: 0.931640625\n",
      "Iteration 130, loss = 0.4501725435256958.\n",
      "Test Accuracy: 0.9267578125\n",
      "Iteration 140, loss = 0.43540579080581665.\n",
      "Test Accuracy: 0.9306640625\n",
      "Iteration 150, loss = 0.2753746807575226.\n",
      "Test Accuracy: 0.93359375\n",
      "Iteration 160, loss = 0.20019002258777618.\n",
      "Test Accuracy: 0.9296875\n",
      "Iteration 170, loss = 0.20039820671081543.\n",
      "Test Accuracy: 0.951171875\n",
      "Iteration 180, loss = 0.2442474663257599.\n",
      "Test Accuracy: 0.9501953125\n",
      "Iteration 190, loss = 0.11188672482967377.\n",
      "Test Accuracy: 0.947265625\n",
      "Iteration 200, loss = 0.17088358104228973.\n",
      "Test Accuracy: 0.9501953125\n"
     ]
    }
   ],
   "source": [
    "# Adiciona Convolução\n",
    "def conv_layer(input, size_in, size_out, name=\"conv\", name2=\"pool\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([5, 5, size_in, size_out], stddev=0.05), name=\"Weights\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"Biases\")\n",
    "        conv = tf.nn.conv2d(input, w, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        relu = tf.nn.relu(conv + b)\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", relu)\n",
    "\n",
    "        return relu\n",
    "\n",
    "# Adiciona Max Pooling\n",
    "def max_pool(input, name):\n",
    "    with tf.name_scope(name):\n",
    "        return tf.nn.max_pool(input, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "# Adiciona totalmente conectada\n",
    "def fc_layer(input, size_in, size_out, name=\"fc\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.05), name=\"Weights\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"Biases\")\n",
    "        relu = tf.nn.relu(tf.matmul(input, w) + b)\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", relu)\n",
    "\n",
    "        return relu\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "# Setup placeholders\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x\")\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 10], name=\"labels\")\n",
    "\n",
    "# Redimensiona a imagem\n",
    "with tf.name_scope('input_reshape'):\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    tf.summary.image('input', x_image, 10)\n",
    "\n",
    "# convoluções e max poolings\n",
    "conv1 = conv_layer(x_image, 1, 32, \"conv1\", \"pool1\")\n",
    "conv1 = max_pool(conv1, \"pool1\")\n",
    "conv2 = conv_layer(conv1, 32, 64, \"conv2\", \"pool2\")\n",
    "conv2 = max_pool(conv2, \"pool2\")\n",
    "\n",
    "# transforma a imagem em um array\n",
    "with tf.name_scope('flatten'):\n",
    "    flattened = tf.reshape(conv2, [-1, 7 * 7 * 64])\n",
    "\n",
    "# totalmente conectada 1\n",
    "fc1 = fc_layer(flattened, 7 * 7 * 64, 1024, \"fc1\")\n",
    "\n",
    "with tf.name_scope('dropout'):\n",
    "    # dropout: elimina algumas unidades; veremos no final se der tempo\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "\n",
    "# totalmente conectada 2\n",
    "fc2 = fc_layer(fc1, 1024, 10, \"fc2\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=fc2, labels=y), name=\"cross_entropy\")\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    # SGD + momentum para otimização\n",
    "    learning_rate = 0.01\n",
    "    momentum = 0.9\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    # minimiza o valor da loss\n",
    "    optimizer = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"test_accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(fc2, 1), tf.argmax(y, 1))\n",
    "    test_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar(\"test_accuracy\", test_accuracy)\n",
    "\n",
    "summ = tf.summary.merge_all()    \n",
    "    \n",
    "with tf.name_scope(\"embeddings\"):\n",
    "    embedding_input = fc1\n",
    "    embedding_size = 1024\n",
    "    embedding = tf.Variable(tf.zeros([1024, embedding_size]), name=\"test_embedding\")\n",
    "    assignment = embedding.assign(embedding_input)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(LOGDIR + 'cnn_emb')\n",
    "    writer.add_graph(sess.graph)\n",
    "\n",
    "    config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n",
    "    embedding_config = config.embeddings.add()\n",
    "    embedding_config.tensor_name = embedding.name\n",
    "    embedding_config.sprite.image_path = LOGDIR + 'sprite_1024.png'\n",
    "    embedding_config.metadata_path = LOGDIR + 'labels_1024.tsv'\n",
    "    embedding_config.sprite.single_image_dim.extend([28, 28])\n",
    "    tf.contrib.tensorboard.plugins.projector.visualize_embeddings(writer, config)\n",
    "\n",
    "for iteration in range(MAX_ITERS):\n",
    "    # obtém o próximo batch de imagens e labels\n",
    "    (images, labels) = mnist.train.next_batch(BATCH_SIZE)\n",
    "    \n",
    "    # executa uma iteração da otimização\n",
    "    (_, iter_loss) = sess.run([optimizer, loss], \n",
    "                          feed_dict={x: images,\n",
    "                                     y: labels,\n",
    "                                     keep_prob: 0.5})  \n",
    "    \n",
    "    # avalia o modelo a cada 10 iterações\n",
    "    if iteration % 10 == 0:\n",
    "        [test_acc, s] = sess.run([test_accuracy, summ], \n",
    "                                 feed_dict={x: mnist.test.images[:n_samples], \n",
    "                                            y: mnist.test.labels[:n_samples],\n",
    "                                            keep_prob: 1.0})\n",
    "        print('Iteration {}, loss = {}.'.format(iteration, iter_loss))\n",
    "        print('Test Accuracy: {0}'.format(test_acc))\n",
    "        writer.add_summary(s, iteration)\n",
    "        \n",
    "    if iteration % 100 == 0:\n",
    "        sess.run(assignment, feed_dict={x: mnist.test.images[:n_samples], \n",
    "                                        y: mnist.test.labels[:n_samples],\n",
    "                                        keep_prob: 1.0})\n",
    "        saver.save(sess, os.path.join(LOGDIR, \"cnn.ckpt\"), iteration)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
